{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Setup and Import Statements\n",
    "\n",
    "This project leverages a variety of Python libraries to handle data manipulation, visualization, machine learning, preprocessing, model evaluation, and addressing class imbalance. Below are the consolidated import statements used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, classification_report, roc_curve)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "This section is responsible for loading the dataset into the environment and providing an initial glimpse of its structure. Proper error handling ensures that the process is robust and user-friendly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data = pd.read_csv('./american_bankruptcy.csv')   \n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please check the file path.\")\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Dataset Structure\n",
    "\n",
    "Understanding the structure of your dataset is crucial for effective data analysis and preprocessing. This section provides insights into the size and composition of the dataset, helping to identify potential issues and plan subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataset\n",
    "print(f\"Dataset contains {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "\n",
    "# Get basic info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate descriptive statistics for the dataset, including count, mean, standard deviation, minimum, maximum, and quartile values for numerical columns, as well as counts and unique values for categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for missing values in each column and print out the columns that have missing values along with the count of missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and duplicates\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(data.isnull().sum()[data.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify and count the number of duplicate rows in the dataset to assess data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Target Variable\n",
    "\n",
    "Understanding the distribution of the target variable is essential in any machine learning project. This section visualizes the distribution of the `status_label` to assess class balance and gain insights into the dataset's composition. Visualizing the target variable aids in identifying potential issues such as class imbalance, which can significantly impact model performance and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target variable\n",
    "sns.countplot(x='status_label', data=data)\n",
    "plt.title('Distribution of Company Status')\n",
    "plt.show()\n",
    "print(\"\\nClass distribution:\")\n",
    "print(data['status_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Preparing the dataset for machine learning involves several crucial preprocessing steps to ensure that the data is clean, consistent, and in a suitable format for modeling. This section outlines the steps taken to preprocess the dataset, including dropping unnecessary columns, handling data types, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data.drop(columns=['company_name'], inplace=True)\n",
    "\n",
    "# Ensure 'year' is numeric\n",
    "data['year'] = pd.to_numeric(data['year'], errors='coerce')\n",
    "\n",
    "# Encode 'status_label' if necessary\n",
    "if data['status_label'].dtype == 'object':\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['status_label'] = label_encoder.fit_transform(data['status_label'])\n",
    "    label_mapping = dict(\n",
    "        zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    print(\"\\nLabel Mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Columns with Negative Values\n",
    "\n",
    "Ensuring data quality is a fundamental step in the data preprocessing pipeline. Negative values in certain columns may indicate data entry errors, invalid measurements, or specific characteristics of the data that need to be addressed. This section identifies which numerical columns contain negative values, allowing for informed decisions on how to handle them in subsequent analysis and modeling steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with negative values\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "negative_values = (data[numeric_cols] < 0).any()\n",
    "print(\"Columns with negative values:\")\n",
    "print(negative_values[negative_values == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Numerical Features with Boxplots\n",
    "\n",
    "Boxplots are an effective way to visualize the distribution of numerical features in a dataset. They help in identifying the presence of outliers, understanding the spread of the data, and comparing distributions across different variables. This section generates boxplots for all numerical features (excluding the target variable) in the dataset, providing a comprehensive overview of their distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical features with boxplots\n",
    "numeric_cols.remove('status_label')\n",
    "num_cols = len(numeric_cols)\n",
    "num_cols_per_row = 2\n",
    "num_rows = math.ceil(num_cols / num_cols_per_row)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=num_rows, ncols=num_cols_per_row, figsize=(15, num_rows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(x=data[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot of {col}')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='status_label')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Select the upper triangle of the correlation matrix\n",
    "upper = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than the threshold (e.g., 0.8)\n",
    "threshold = 0.8\n",
    "to_drop = [column for column in upper.columns if any(\n",
    "    upper[column] > threshold)]\n",
    "\n",
    "print(f\"Features to drop due to high correlation (> {threshold}): {to_drop}\")\n",
    "\n",
    "# Drop the features from X\n",
    "X_filtered = X.drop(columns=to_drop)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection and Removal in the Majority Class\n",
    "\n",
    "Outliers can significantly skew the results of data analysis and machine learning models. This section focuses on identifying and removing outliers from the majority class in the dataset using the Interquartile Range (IQR) method. By targeting the majority class, we aim to refine the dataset without compromising the integrity of the minority class, which is crucial for maintaining class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers based on IQR\n",
    "def remove_outliers_IQR(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Separate the data by class\n",
    "data_majority = data[data['status_label'] == 0]\n",
    "data_minority = data[data['status_label'] == 1]\n",
    "\n",
    "print(\"Original majority class shape:\", data_majority.shape)\n",
    "print(\"Original minority class shape:\", data_minority.shape)\n",
    "\n",
    "# Apply the outlier removal to the majority class only\n",
    "for col in numeric_cols:\n",
    "    data_majority = remove_outliers_IQR(data_majority, col)\n",
    "\n",
    "print(\"Majority class shape after outlier removal:\", data_majority.shape)\n",
    "\n",
    "# Recombine the data\n",
    "data = pd.concat([data_majority, data_minority], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Verify class distribution after outlier removal\n",
    "print(\"Class distribution after outlier removal:\")\n",
    "print(data['status_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset into Training and Testing Sets\n",
    "\n",
    "After preprocessing the data and ensuring its quality, the next crucial step is to prepare it for machine learning model training and evaluation. This involves separating the dataset into features and the target variable, followed by splitting the data into training and testing subsets. Proper splitting ensures that the model can generalize well to unseen data and provides an unbiased evaluation of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop(columns=['status_label'])\n",
    "y = data['status_label']\n",
    "\n",
    "# Split the data into training and testing sets (before scaling and PCA)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Class distribution in y_train:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling with StandardScaler\n",
    "\n",
    "Scaling features is a critical preprocessing step in machine learning, especially for algorithms that are sensitive to the scale of input data (e.g., Support Vector Machines, Logistic Regression, and Neural Networks). This section applies feature scaling to the dataset using `StandardScaler` from Scikit-learn. The scaler is fitted on the training data to prevent data leakage and then applied to both the training and testing sets to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with Principal Component Analysis (PCA)\n",
    "\n",
    "Dimensionality reduction is a crucial step in the machine learning pipeline, especially when dealing with high-dimensional data. It helps in simplifying models, reducing computational costs, and mitigating the risk of overfitting. This section applies **Principal Component Analysis (PCA)** to the scaled features to reduce the number of dimensions while retaining 90% of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA (fit PCA on training data only)\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Reduced number of features after PCA: {X_train_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve PCA components\n",
    "pca_components = pca.components_\n",
    "\n",
    "# Get the number of principal components\n",
    "num_pcs = pca_components.shape[0]\n",
    "\n",
    "# Create names for principal components\n",
    "pc_names = [f'PC{i+1}' for i in range(num_pcs)]\n",
    "\n",
    "# Retrieve original feature names\n",
    "original_feature_names = X.columns.tolist()\n",
    "\n",
    "# Create a DataFrame for PCA components with original feature names as columns\n",
    "pca_df = pd.DataFrame(\n",
    "    pca_components, columns=original_feature_names, index=pc_names)\n",
    "\n",
    "# Transpose for better readability\n",
    "pca_df = pca_df.transpose()\n",
    "\n",
    "# Display the PCA components\n",
    "print(\"PCA Components (Principal Components):\")\n",
    "print(pca_df)\n",
    "\n",
    "# Function to plot top contributing features for each principal component\n",
    "\n",
    "def plot_top_features(pca, feature_names, top_n=5):\n",
    "    for i in range(pca.n_components_):\n",
    "        component = pca.components_[i]\n",
    "        abs_component = np.abs(component)\n",
    "        top_indices = abs_component.argsort()[::-1][:top_n]\n",
    "        top_features = [feature_names[j] for j in top_indices]\n",
    "        top_values = component[top_indices]\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.barplot(x=top_features, y=top_values, palette='viridis')\n",
    "        plt.title(f'Top {top_n} Features for Principal Component {i+1}')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Loading Coefficient')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot top 5 features for each principal component\n",
    "plot_top_features(pca, original_feature_names, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance with SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "Class imbalance is a common issue in machine learning, particularly in classification tasks where one class significantly outnumbers another. Imbalanced datasets can lead to biased models that favor the majority class, resulting in poor predictive performance for the minority class. To address this, **SMOTE (Synthetic Minority Over-sampling Technique)** is employed to create synthetic examples of the minority class, thereby balancing the class distribution and enhancing the model's ability to generalize effectively.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section demonstrates how to handle class imbalance in the training dataset using SMOTE. By resampling the training data, we aim to improve the classifier's performance on the minority class without altering the testing data, ensuring that the evaluation remains unbiased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling class imbalance with SMOTE\n",
    "print(\"Class distribution in y_train before SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_pca, y_train)\n",
    "\n",
    "print(\"Class distribution in y_train after SMOTE:\")\n",
    "print(y_train_res.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Logistic Regression Model\n",
    "\n",
    "With the dataset now preprocessed, scaled, and balanced, the next step is to build and train a machine learning model. **Logistic Regression** is a fundamental classification algorithm that is widely used due to its simplicity, interpretability, and efficiency. This section demonstrates how to create, train, and make predictions using a Logistic Regression model on the prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = log_reg.predict(X_test_pca)\n",
    "y_prob_lr = log_reg.predict_proba(X_test_pca)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Logistic Regression Model\n",
    "\n",
    "After training the Logistic Regression model, it's essential to assess its performance to ensure that it generalizes well to unseen data. This section introduces an evaluation function that computes key performance metrics and visualizes the Receiver Operating Characteristic (ROC) curve. By systematically evaluating the model, we can gain insights into its strengths and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(y_test, y_pred, y_prob, model_name):\n",
    "    print(f\"\\nEvaluation Metrics for {model_name}:\")\n",
    "    print(\"-------------------------------------\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(\n",
    "        fpr, tpr, label=f'{model_name} (AUC = {roc_auc_score(y_test, y_prob):.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "evaluate_model(y_test, y_pred_lr, y_prob_lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True, class_weight='balanced', random_state=42)\n",
    "svc.fit(X_train_res, y_train_res)\n",
    "y_pred_svc = svc.predict(X_test_pca)\n",
    "y_prob_svc = svc.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "evaluate_model(y_test, y_pred_svc, y_prob_svc, 'Support Vector Machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "y_pred_rf = rf.predict(X_test_pca)\n",
    "y_prob_rf = rf.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "evaluate_model(y_test, y_pred_rf, y_prob_rf, 'Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Logistic Regression Model\n",
    "\n",
    "\n",
    "The logistic regression model achieved an accuracy of 85.74% in predicting company bankruptcy, with a precision of 0.54, recall of 0.66, and F1-score of 0.59 for the \"bankrupt\" class. The model performs well in predicting \"non-bankrupt\" companies, with high precision (0.93) and F1-score (0.91) for this class. However, it struggles to accurately identify all bankrupt companies, as reflected in the lower precision and F1-score for the \"bankrupt\" class. The ROC curve, with an AUC of 0.8357, shows that the model can reasonably distinguish between bankrupt and non-bankrupt companies, though not perfectly. The lower F1-score for bankrupt companies suggests that the model may be limited in capturing complex relationships in financial data. This could be due to the linear nature of logistic regression, which may not fully capture the nonlinear patterns typically found in financial datasets/features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "We can compare results across models to determine how best to evaluate company bankrupcty predictions. We will evaluate SVM and Random Forest using the same metrics: accuracy, precision, recall, F1-score, and ROC-AUC. Since Random Forest captures nonlinear patterns better, it may improve recall and F1-score for the \"bankrupt\" class. Visualizing ROC curves for each model can highlight their class separation abilities. Additionally, a precision-recall curve for each model will help assess performance, especially for imbalanced classes. By examining these metrics and curves side-by-side, we can identify the model best suited for our data and goals, especially in terms of identifying bankrupt companies accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checklist for HTML:**\n",
    "\n",
    "- 1+ Data Preprocessing Method Implemented (DONE)\n",
    "- 1+ ML Algorithms/Models Implemented (DONE - Logistic Regression, SVM, Random Forest)\n",
    "- CS 4641: Supervised Learning Method Implemented (DONE - Logistic Regression, SVM, Random Forest)\n",
    "- Visualizations (DONE)\n",
    "- Quantitative Metrics (DONE)\n",
    "- Analysis of 1+ Algorithm/Model (DONE)\n",
    "- Next Steps (DONE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
